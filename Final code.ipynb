{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the important libraries \n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml import clustering\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as fn\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import concat_ws, col, lit\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv fike in dataframe\n",
    "data = spark.read.csv('Accidents.csv',header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+--------+-------------------+-------------------+---------+----------+-------+-------+------------+--------------------+------+--------------------+----+------------+----------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+\n",
      "|  ID|  Source|  TMC|Severity|         Start_Time|           End_Time|Start_Lat| Start_Lng|End_Lat|End_Lng|Distance(mi)|         Description|Number|              Street|Side|        City|    County|State|   Zipcode|Country|  Timezone|Airport_Code|  Weather_Timestamp|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Visibility(mi)|Wind_Direction|Wind_Speed(mph)|Precipitation(in)|Weather_Condition|Amenity| Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station| Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|\n",
      "+----+--------+-----+--------+-------------------+-------------------+---------+----------+-------+-------+------------+--------------------+------+--------------------+----+------------+----------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+\n",
      "| A-1|MapQuest|201.0|       3|2016-02-08 05:46:00|2016-02-08 11:00:00|39.865147|-84.058723|   null|   null|        0.01|Right lane blocke...|  null|              I-70 E|   R|      Dayton|Montgomery|   OH|     45424|     US|US/Eastern|        KFFO|2016-02-08 05:58:00|          36.9|         null|       91.0|       29.68|          10.0|          Calm|           null|             0.02|       Light Rain|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|         Night|         Night|            Night|                Night|\n",
      "| A-2|MapQuest|201.0|       2|2016-02-08 06:07:59|2016-02-08 06:37:59|39.928059|-82.831184|   null|   null|        0.01|Accident on Brice...|2584.0|            Brice Rd|   L|Reynoldsburg|  Franklin|   OH|43068-3402|     US|US/Eastern|        KCMH|2016-02-08 05:51:00|          37.9|         null|      100.0|       29.65|          10.0|          Calm|           null|              0.0|       Light Rain|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|         Night|         Night|            Night|                  Day|\n",
      "| A-3|MapQuest|201.0|       2|2016-02-08 06:49:27|2016-02-08 07:19:27|39.063148|-84.032608|   null|   null|        0.01|Accident on OH-32...|  null|      State Route 32|   R|Williamsburg|  Clermont|   OH|     45176|     US|US/Eastern|        KI69|2016-02-08 06:56:00|          36.0|         33.3|      100.0|       29.67|          10.0|            SW|            3.5|             null|         Overcast|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|          True|       False|         Night|         Night|              Day|                  Day|\n",
      "| A-4|MapQuest|201.0|       3|2016-02-08 07:23:34|2016-02-08 07:53:34|39.747753|-84.205582|   null|   null|        0.01|Accident on I-75 ...|  null|              I-75 S|   R|      Dayton|Montgomery|   OH|     45417|     US|US/Eastern|        KDAY|2016-02-08 07:38:00|          35.1|         31.0|       96.0|       29.64|           9.0|            SW|            4.6|             null|    Mostly Cloudy|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|         Night|           Day|              Day|                  Day|\n",
      "| A-5|MapQuest|201.0|       2|2016-02-08 07:39:07|2016-02-08 08:09:07|39.627781|-84.188354|   null|   null|        0.01|Accident on McEwe...|  null|Miamisburg Center...|   R|      Dayton|Montgomery|   OH|     45459|     US|US/Eastern|        KMGY|2016-02-08 07:53:00|          36.0|         33.3|       89.0|       29.65|           6.0|            SW|            3.5|             null|    Mostly Cloudy|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|          True|       False|           Day|           Day|              Day|                  Day|\n",
      "| A-6|MapQuest|201.0|       3|2016-02-08 07:44:26|2016-02-08 08:14:26| 40.10059|-82.925194|   null|   null|        0.01|Accident on I-270...|  null|      Westerville Rd|   R| Westerville|  Franklin|   OH|     43081|     US|US/Eastern|        KCMH|2016-02-08 07:51:00|          37.9|         35.5|       97.0|       29.63|           7.0|           SSW|            3.5|             0.03|       Light Rain|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "| A-7|MapQuest|201.0|       2|2016-02-08 07:59:35|2016-02-08 08:29:35|39.758274|-84.230507|   null|   null|         0.0|Accident on Oakri...| 376.0|      N Woodward Ave|   R|      Dayton|Montgomery|   OH|45417-2476|     US|US/Eastern|        KDAY|2016-02-08 07:56:00|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|             null|         Overcast|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "| A-8|MapQuest|201.0|       3|2016-02-08 07:59:58|2016-02-08 08:29:58|39.770382|-84.194901|   null|   null|        0.01|Accident on I-75 ...|  null|           N Main St|   R|      Dayton|Montgomery|   OH|     45405|     US|US/Eastern|        KDAY|2016-02-08 07:56:00|          34.0|         31.0|      100.0|       29.66|           7.0|           WSW|            3.5|             null|         Overcast|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "| A-9|MapQuest|201.0|       2|2016-02-08 08:00:40|2016-02-08 08:30:40|39.778061|-84.172005|   null|   null|         0.0|Accident on Notre...|  99.0|      Notre Dame Ave|   L|      Dayton|Montgomery|   OH|45404-1923|     US|US/Eastern|        KFFO|2016-02-08 07:58:00|          33.3|         null|       99.0|       29.67|           5.0|            SW|            1.2|             null|    Mostly Cloudy|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "|A-10|MapQuest|201.0|       3|2016-02-08 08:10:04|2016-02-08 08:40:04| 40.10059|-82.925194|   null|   null|        0.01|Right hand should...|  null|      Westerville Rd|   R| Westerville|  Franklin|   OH|     43081|     US|US/Eastern|        KCMH|2016-02-08 08:28:00|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|             0.02|       Light Rain|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "|A-11|MapQuest|201.0|       3|2016-02-08 08:14:42|2016-02-08 08:44:42|39.952812|-83.119293|   null|   null|        0.01|Accident on I-270...|  null|         Outerbelt S|   R|    Columbus|  Franklin|   OH|     43228|     US|US/Eastern|        KTZR|2016-02-08 07:50:00|          35.6|         30.7|       93.0|       29.64|           5.0|           WNW|            5.8|             null|             Rain|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "|A-12|MapQuest|241.0|       3|2016-02-08 08:21:27|2016-02-08 08:51:27|39.932709| -82.83091|   null|   null|        0.01|One lane blocked ...|  null|              I-70 E|   R|Reynoldsburg|  Franklin|   OH|     43068|     US|US/Eastern|        KCMH|2016-02-08 08:28:00|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|             0.02|       Light Rain|  False|False|   False|   False|    True|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "|A-13|MapQuest|201.0|       2|2016-02-08 08:36:34|2016-02-08 09:06:34|39.737633|-84.149933|   null|   null|         0.0|Accident on Rever...|  99.0|      Watervliet Ave|   R|      Dayton|Montgomery|   OH|45420-1863|     US|US/Eastern|        KFFO|2016-02-08 08:28:00|          33.8|         null|      100.0|       29.63|           3.0|            SW|            2.3|             null|         Overcast|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "|A-14|MapQuest|201.0|       2|2016-02-08 08:37:07|2016-02-08 09:07:07| 39.79076|-84.241547|   null|   null|        0.01|Accident on Salem...|3198.0|           Salem Ave|   L|      Dayton|Montgomery|   OH|45406-2708|     US|US/Eastern|        KDAY|2016-02-08 08:56:00|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|             null|    Mostly Cloudy|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|          True|       False|           Day|           Day|              Day|                  Day|\n",
      "|A-15|MapQuest|201.0|       2|2016-02-08 08:39:43|2016-02-08 09:09:43|39.972038|-82.913521|   null|   null|        0.01|Accident on OH-16...|3280.0|          E Broad St|   L|    Columbus|  Franklin|   OH|43213-1006|     US|US/Eastern|        KCMH|2016-02-08 08:28:00|          37.4|         33.8|      100.0|       29.62|           3.0|           SSW|            4.6|             0.02|       Light Rain|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|          True|       False|           Day|           Day|              Day|                  Day|\n",
      "|A-16|MapQuest|201.0|       2|2016-02-08 08:43:20|2016-02-08 09:13:20|39.745888| -84.17041|   null|   null|        0.01|Accident on Wayne...| 100.0|         Glencoe Ave|   R|      Dayton|Montgomery|   OH|45410-1721|     US|US/Eastern|        KFFO|2016-02-08 08:28:00|          33.8|         null|      100.0|       29.63|           3.0|            SW|            2.3|             null|         Overcast|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "|A-17|MapQuest|201.0|       2|2016-02-08 08:53:17|2016-02-08 09:23:17|39.748329|-84.224007|   null|   null|        0.01|Accident on James...|  null|S James H McGee Blvd|   R|      Dayton|Montgomery|   OH|     45402|     US|US/Eastern|        KFFO|2016-02-08 08:58:00|          35.6|         null|       99.0|       29.65|           7.0|           WSW|            2.3|             null|    Mostly Cloudy|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "|A-18|MapQuest|201.0|       2|2016-02-08 09:24:37|2016-02-08 09:54:37|39.752174|-84.239952|   null|   null|         0.0|Accident on Delph...|3001.0|         Delphos Ave|   R|      Dayton|Montgomery|   OH|45417-1727|     US|US/Eastern|        KDAY|2016-02-08 08:56:00|          36.0|         31.1|       89.0|       29.65|          10.0|            NW|            5.8|             null|    Mostly Cloudy|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "|A-19|MapQuest|201.0|       2|2016-02-08 09:25:17|2016-02-08 09:55:17|39.740669|-84.184135|   null|   null|        0.01|Accident on Stewa...| 440.0|          Rubicon St|   L|      Dayton|Montgomery|   OH|45409-2659|     US|US/Eastern|        KFFO|2016-02-08 09:38:00|          37.4|         32.1|       93.0|       29.63|          10.0|           WSW|            6.9|             null|         Overcast|  False|False|    True|   False|   False|  False|  False|     False|  False|False|          False|          True|       False|           Day|           Day|              Day|                  Day|\n",
      "|A-20|MapQuest|201.0|       2|2016-02-08 09:35:35|2016-02-08 10:05:35|39.790703|-84.244461|   null|   null|        0.01|Accident on Hillc...|3499.0|     W Hillcrest Ave|   R|      Dayton|Montgomery|   OH|45406-2640|     US|US/Eastern|        KDAY|2016-02-08 09:56:00|          36.0|         30.3|       89.0|       29.65|          10.0|          West|            6.9|             null|    Mostly Cloudy|  False|False|   False|   False|   False|  False|  False|     False|  False|False|          False|         False|       False|           Day|           Day|              Day|                  Day|\n",
      "+----+--------+-----+--------+-------------------+-------------------+---------+----------+-------+-------+------------+--------------------+------+--------------------+----+------------+----------+-----+----------+-------+----------+------------+-------------------+--------------+-------------+-----------+------------+--------------+--------------+---------------+-----------------+-----------------+-------+-----+--------+--------+--------+-------+-------+----------+-------+-----+---------------+--------------+------------+--------------+--------------+-----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#function to drop all columns that have null values in all the rows\n",
    "import pyspark.sql.functions as F\n",
    "def drop_null_columns(df):\n",
    "    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "    to_drop = [k for k, v in null_counts.items() if v == data.count()]\n",
    "    df = df.drop(*to_drop)\n",
    "    return df\n",
    "\n",
    "# Drops column b2, because it contains null values\n",
    "drop_null_columns(data).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#count plot for source column\n",
    "source = data.select(\"Source\").collect()\n",
    "source = [s.Source for s in source]\n",
    "sns.countplot(x= source)\n",
    "sns.despine(left = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see most of the accidents were reported by Mapquest, and then bing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#na values in each column\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop End_lat and End_lng as they have many null values \n",
    "data = data.drop('End_Lat')\n",
    "data = data.drop('End_Lng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean for numeric columns\n",
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "col_list = ['Distance(mi)','Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)','Precipitation(in)']\n",
    "df_float_mean = data.select([_mean(col(c)).alias('mean_'+str(c)) for c in col_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null values with mean\n",
    "mean_Distance = df_float_mean.select('mean_Distance(mi)').collect()[0]['mean_Distance(mi)']\n",
    "mean_Temperature = df_float_mean.select('mean_Temperature(F)').collect()[0]['mean_Temperature(F)']\n",
    "mean_Wind_Chill = df_float_mean.select('mean_Wind_Chill(F)').collect()[0]['mean_Wind_Chill(F)']\n",
    "mean_Humidity = df_float_mean.select('mean_Humidity(%)').collect()[0]['mean_Humidity(%)']\n",
    "mean_Pressure = df_float_mean.select('mean_Pressure(in)').collect()[0]['mean_Pressure(in)']\n",
    "mean_Visibility = df_float_mean.select('mean_Visibility(mi)').collect()[0]['mean_Visibility(mi)']\n",
    "mean_Wind_Speed = df_float_mean.select('mean_Wind_Speed(mph)').collect()[0]['mean_Wind_Speed(mph)']\n",
    "mean_Precipitation = df_float_mean.select('mean_Precipitation(in)').collect()[0]['mean_Precipitation(in)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna({'Temperature(F)':mean_Temperature,'Wind_Chill(F)':mean_Wind_Chill,'Humidity(%)':mean_Humidity,'Pressure(in)':mean_Pressure,'Visibility(mi)':mean_Visibility,'Wind_Speed(mph)':mean_Wind_Speed,'Precipitation(in)':mean_Precipitation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#na values in each column\n",
    "data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all rows that contain null values and drop Number column as it has 2/3rd values as null\n",
    "data = data.drop('Number')\n",
    "data = data.na.drop(subset=[\"City\"])\n",
    "data = data.na.drop(subset=[\"State\"])\n",
    "data = data.na.drop(subset=[\"Zipcode\"])\n",
    "data = data.na.drop(subset=[\"Timezone\"])\n",
    "data = data.na.drop(subset=[\"Airport_code\"])\n",
    "data = data.na.drop(subset=[\"Weather_Timestamp\"])\n",
    "data = data.na.drop(subset=[\"Sunrise_Sunset\"])\n",
    "data = data.na.drop(subset=[\"Civil_Twilight\"])\n",
    "data = data.na.drop(subset=[\"Nautical_Twilight\"])\n",
    "data = data.na.drop(subset=[\"Astronomical_Twilight\"])\n",
    "data = data.na.drop(subset=[\"Wind_Direction\"])\n",
    "data = data.na.drop(subset=[\"Weather_Condition\"])\n",
    "data = data.na.drop(subset=[\"TMC\"])\n",
    "data = data.na.drop(subset=[\"Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of acc per state\n",
    "num_Acc = data.groupBy('State').count().alias(\"counts\")\n",
    "\n",
    "num_Acc= num_Acc.toDF('State','count_state').toPandas()\n",
    "\n",
    "plt.figure(figsize=(25,15))\n",
    "sns.set(style=\"white\")\n",
    "g= sns.barplot(x=\"State\",y=\"count_state\",data=num_Acc,color=(0.21569, 0.21569 ,0.21569))\n",
    "\n",
    "g.set_xticklabels(rotation=90,labels=num_Acc['State'])\n",
    "g.set(yticklabels=[])\n",
    "g.set_ylabel('')\n",
    "g.axes.set_title(\"Number of Acidents per State\",fontsize=20)\n",
    "g.set_xlabel('State',fontsize=14)\n",
    "sns.despine(left=True)\n",
    "\n",
    "locs, labels = plt.xticks() # get the current tick locations and labels\n",
    "\n",
    "for loc, label in zip(locs, labels):\n",
    "    count = num_Acc.iloc[loc].count_state\n",
    "    plt.text(loc, count-8, '{:0.0f}'.format(count), ha='center', va='bottom' ,color = 'black',size=11)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 20 Counts with max accidents \n",
    "coynty_max = data.groupBy('County').count().alias(\"count_county\")\n",
    "coynty_max= coynty_max.toDF('County','count_county').toPandas().sort_values(by = 'count_county',ascending = False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "sns.set(style=\"white\")\n",
    "g= sns.barplot(x=\"County\",y=\"count_county\",data=coynty_max,color=(0.21569, 0.21569 ,0.21569))\n",
    "\n",
    "g.set_xticklabels(rotation=90,labels=coynty_max['County'])\n",
    "g.set(yticklabels=[])\n",
    "g.set_ylabel('')\n",
    "g.axes.set_title(\"Countys with max num of accidents\",fontsize=20)\n",
    "g.set_xlabel('County',fontsize=14)\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 20 Weather_Conditions with max accidents \n",
    "conditions = data.groupBy('Weather_Condition').count().alias(\"count_weather\")\n",
    "conditions= conditions.toDF('Weather_Condition','count_weather').toPandas().sort_values(by = 'count_weather',ascending = False)[:20]\n",
    "plt.figure(figsize=(25,15))\n",
    "sns.set(style=\"white\")\n",
    "g= sns.barplot(x=\"Weather_Condition\",y=\"count_weather\",data=conditions,color=(0.21569, 0.21569 ,0.21569))\n",
    "\n",
    "g.set_xticklabels(rotation=90,labels=conditions['Weather_Condition'])\n",
    "g.set(yticklabels=[])\n",
    "g.set_ylabel('')\n",
    "g.axes.set_title(\"Countys with max num of accidents\",fontsize=20)\n",
    "g.set_xlabel('Weather_Condition',fontsize=14)\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly\n",
    "#graphical representation of num of accidents state wise\n",
    "import plotly.graph_objects as go\n",
    "state = data.select('State').collect()\n",
    "state = [s.State for s in state]\n",
    "df_st_ct = pd.value_counts(pd.Series(state))\n",
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations=df_st_ct.index,\n",
    "    z = df_st_ct.values.astype(float),  # Data to be color-coded\n",
    "    locationmode = 'USA-states',     # set of locations match entries in `locations`\n",
    "    colorscale = 'amp',\n",
    "    colorbar_title = \"Count\",\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'US Accidents by State',\n",
    "    geo_scope='usa', # limite map scope to USA\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count plot for severity\n",
    "severity_dist = data.groupBy('Severity').count().alias(\"count_Severity\")\n",
    "severity_dist= severity_dist.toDF('Severity','count_Severity').toPandas()\n",
    "plt.figure(figsize=(25,15))\n",
    "sns.set(style=\"white\")\n",
    "g= sns.barplot(x=\"Severity\",y=\"count_Severity\",data=severity_dist,color=(0.21569, 0.21569 ,0.21569))\n",
    "\n",
    "\n",
    "g.set(yticklabels=[])\n",
    "g.set_ylabel('')\n",
    "g.axes.set_title(\"Number of Acidents per State\",fontsize=20)\n",
    "g.set_xlabel('Severity',fontsize=14)\n",
    "sns.despine(left=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of data points with severity =1 and 4 are very less thus replace them with 2 and 3\n",
    "data = data.withColumn(\"Severity\", F.when(F.col(\"Severity\")=='1', 2).otherwise(F.col(\"Severity\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"Severity\", F.when(F.col(\"Severity\")==4, 3).otherwise(F.col(\"Severity\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter(col(\"Severity\")=='2').count()/data.filter(col(\"Severity\")=='3').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The data is very huge so we have used just a part of it to run models. We have stratified sampling for selecting data\n",
    "sampled_data = data.sampleBy(\"Severity\", fractions={'2': 0.3, '3': 0.3}, seed=0)\n",
    "sampled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The ratio of num of col with severity =2 /severity =3 remains same even after sampling \n",
    "sampled_data.filter(col(\"Severity\")==2).count()/sampled_data.filter(col(\"Severity\")==3).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast distance to doubletype\n",
    "from pyspark.sql.types import *\n",
    "sampled_data = sampled_data.withColumn('Distance(mi)', col('Distance(mi)').cast(DoubleType())).withColumn('Temperature(F)', col('Temperature(F)').cast(DoubleType())).withColumn('Wind_Chill(F)', col('Humidity(%)').cast(DoubleType())).withColumn('Humidity(%)', col('Wind_Chill(F)').cast(DoubleType())).withColumn('Pressure(in)', col('Pressure(in)').cast(DoubleType())).withColumn('Visibility(mi)', col('Visibility(mi)').cast(DoubleType())).withColumn('Wind_Speed(mph)', col('Wind_Speed(mph)').cast(DoubleType())).withColumn('Precipitation(in)', col('Precipitation(in)').cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a correlation matrix \n",
    "corr = sampled_data.toPandas().corr()\n",
    "plt.figure(figsize=(25,15))\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True,\n",
    "    annot = True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data.toPandas().to_csv('sample_data1.csv',index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = sampled_data.withColumn('Severity', col('Severity').cast(IntegerType()))\n",
    "sampled_data = sampled_data.withColumn(\"Severity\", fn.when(fn.col(\"Severity\")==2, 1).otherwise(0))\n",
    "train_data, validation_data= sampled_data.randomSplit([0.6,0.4], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector assembler to create a feature vector that would act as input First for categorical col then numerical and then combine the two. \n",
    "#We have used onehotencoder to convert categorical variable to numeric representation\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "col_list = ['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','TMC',\n",
    " 'Side',\n",
    " 'City',\n",
    " 'County',\n",
    " 'State',\n",
    " 'Timezone',\n",
    " 'Airport_Code',\n",
    " 'Weather_Condition',\n",
    " 'Sunrise_Sunset',\n",
    " 'Astronomical_Twilight'\n",
    "]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=\"{0}_indexed\".format(column), handleInvalid = \"keep\")for column in col_list]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[\n",
    "        \"{0}_encoded\".format(indexer.getOutputCol()) for indexer in indexers]\n",
    ")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=encoder.getOutputCols(),\n",
    "    outputCol=\"categorical\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "pipeline_t = pipeline.fit(train_data)\n",
    "train_data = pipeline_t.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = ['Temperature(F)',\n",
    " 'Wind_Chill(F)',\n",
    " 'Humidity(%)',\n",
    " 'Pressure(in)',\n",
    " 'Visibility(mi)',\n",
    " 'Wind_Speed(mph)',\n",
    " 'Precipitation(in)',\n",
    "'Distance(mi)']\n",
    "assemblerNum = VectorAssembler(inputCols = num_cols, outputCol = \"num\")\n",
    "\n",
    "pipelineNum = Pipeline(stages = [assemblerNum])\n",
    "pipelineNum_t = pipelineNum.fit(train_data)\n",
    "train_data = pipelineNum_t.transform(train_data)\n",
    "\n",
    "\n",
    "assembler1 = VectorAssembler(inputCols = [\"categorical\", \"num\"], outputCol = \"features\")\n",
    "\n",
    "pipeline1 = Pipeline(stages = [assembler1])\n",
    "pipeline1_t = pipeline1.fit(train_data)\n",
    "train_data = pipeline1_t.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pipeline_t.transform(validation_data)\n",
    "validation_data = pipelineNum_t.transform(validation_data)\n",
    "validation_data = pipeline1_t.transform(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "#Renaming the severity column to label and replacing the values of labels\n",
    "sampled_data1 = sampled_data.withColumn('Severity', sampled_data['Severity'].cast(IntegerType()))\n",
    "sampled_data1 = sampled_data.withColumnRenamed('Severity', 'label')\n",
    "sampled_data1=sampled_data.withColumn(\"label\", F.when(F.col(\"label\")==2, 0).otherwise(F.col(\"label\")))\n",
    "sampled_data1=sampled_data.withColumn(\"label\", F.when(F.col(\"label\")==3, 1).otherwise(F.col(\"label\")))\n",
    "train_data1, test_data1 = sampled_data1.randomSplit([0.7, 0.3], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree classification model and binary classification to evaluate its performance\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dtModel = dt.fit(train_data1)\n",
    "predictions = dtModel.transform(test_data1)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(maxDepth=3)\n",
    "dtModel = dt.fit(train_data1)\n",
    "predictions = dtModel.transform(test_data1)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(maxDepth=5, maxBins=32)\n",
    "dtModel = dt.fit(train_data)\n",
    "predictions = dtModel.transform(test_data)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(maxDepth=6, maxBins=32)\n",
    "dtModel = dt.fit(train_data1)\n",
    "predictions = dtModel.transform(test_data1)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(maxDepth=7, maxBins=50)\n",
    "dtModel = dt.fit(train_data1)\n",
    "predictions = dtModel.transform(test_data1)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes classification model and binary classification to evaluate its performance\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\",smoothing=1.0)\n",
    "model = nb.fit(train_data1)\n",
    "predictions = model.transform(test_data1)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions,{evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\",smoothing=0.5)\n",
    "model = nb.fit(train_data1)\n",
    "predictions = model.transform(test_data1)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions,{evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\",smoothing=0.4)\n",
    "model = nb.fit(train_data1)\n",
    "predictions = model.transform(test_data1)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions,{evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\",smoothing=0.2)\n",
    "model = nb.fit(train_data1)\n",
    "predictions = model.transform(test_data1)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions,{evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\",smoothing=0.1)\n",
    "model = nb.fit(train_data1)\n",
    "predictions = model.transform(test_data1)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions,{evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest and binary classification to evaluate its performance\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"Severity\", featuresCol=\"features\", numTrees = 100, maxDepth= 5, impurity = 'gini')\n",
    "rf_pipeline1 = Pipeline(stages=[rf])\n",
    "model = rf_pipeline1.fit(train_data)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bce = BinaryClassificationEvaluator(labelCol = 'Severity')\n",
    "print(bce.evaluate(model.transform(validation_data)))\n",
    "\n",
    "model_stage = model.stages[-1]\n",
    "pd.DataFrame(list(zip(train_data.columns[2:], model_stage.featureImportances.toArray())),\n",
    "            columns = ['feature', 'importance']).sort_values('importance', ascending = False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Severity\", featuresCol=\"features\", numTrees = 100, maxDepth= 5, impurity = 'gini')\n",
    "rf_pipeline1 = Pipeline(stages=[rf])\n",
    "model = rf_pipeline1.fit(train_data)\n",
    "\n",
    "bce = BinaryClassificationEvaluator(labelCol = 'Severity')\n",
    "print(bce.evaluate(model.transform(validation_data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier(labelCol=\"Severity\", featuresCol=\"features\", numTrees = 100, maxDepth= 5, impurity = 'entropy')\n",
    "rf_pipeline2 = Pipeline(stages=[rf1])\n",
    "model_rf = rf_pipeline2.fit(train_data)\n",
    "\n",
    "bce = BinaryClassificationEvaluator(labelCol = 'Severity')\n",
    "print(bce.evaluate(model_rf.transform(validation_data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier(labelCol=\"Severity\", featuresCol=\"features\", numTrees = 200, maxDepth= 6, impurity = 'entropy')\n",
    "rf_pipeline2 = Pipeline(stages=[rf1])\n",
    "model_rf = rf_pipeline2.fit(train_data)\n",
    "\n",
    "bce = BinaryClassificationEvaluator(labelCol = 'Severity')\n",
    "print(bce.evaluate(model_rf.transform(validation_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBT Classifier binary classification to evaluate its performance\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"Severity\", featuresCol=\"features\")\n",
    "gbt_pipeline = Pipeline(stages=[gbt])\n",
    "model_gbt = gbt_pipeline.fit(train_data)\n",
    "\n",
    "bce = BinaryClassificationEvaluator(labelCol=\"Severity\")\n",
    "bce.evaluate(model_gbt.transform(validation_data))\n",
    "\n",
    "model_stage = model_gbt.stages[-1]\n",
    "pd.DataFrame(list(zip(train_data.columns[2:], model_stage.featureImportances.toArray())),\n",
    "            columns = ['feature', 'importance']).sort_values('importance', ascending = Fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression  binary classification to evaluate its performance\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('Severity').\\\n",
    "    setFeaturesCol('features').\\\n",
    "    setRegParam(0.0).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.0)\n",
    "lr_pipeline = Pipeline(stages =[lr])\n",
    "lr_fit  = lr_pipeline.fit(train_data)\n",
    "\n",
    "bce = BinaryClassificationEvaluator(labelCol=\"Severity\")\n",
    "bce.evaluate(lr_fit.transform(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('Severity').\\\n",
    "    setFeaturesCol('features').\\\n",
    "    setRegParam(0.3).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.2)\n",
    "lr_pipeline = Pipeline(stages =[lr])\n",
    "lr_fit  = lr_pipeline.fit(train_data)\n",
    "\n",
    "bce = BinaryClassificationEvaluator(labelCol=\"Severity\")\n",
    "bce.evaluate(lr_fit.transform(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('Severity').\\\n",
    "    setFeaturesCol('features').\\\n",
    "    setRegParam(0.1).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.3)\n",
    "lr_pipeline = Pipeline(stages =[lr])\n",
    "lr_fit  = lr_pipeline.fit(train_data)\n",
    "\n",
    "bce = BinaryClassificationEvaluator(labelCol=\"Severity\")\n",
    "bce.evaluate(lr_fit.transform(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
